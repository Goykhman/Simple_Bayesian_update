# Simple Bayesian update

Simple Bayesian update of the coin toss probabilities.

Our ignorance of the outcome of the coin toss is encoded in various probabilities with which we believe in different models (each model itself is in turn probabilistic, characterized by what is known as the conditional probabilities) describing possible outcomes (landing heads or tails) of the coin toss.

We start with random prior believes in various probabilistic models of the coin toss. That is, each model is initially assigned a randomly chosen prior probability, with the total of all of the prior probabilities being equal to one (alternatively, among other options, we could have assigned, for instance, a uniform prior probability distribution believes in each of the models).

After an outcome of each coin toss is observed we update our believes in these probabilistic models. Of course, the posterior probability after the toss i is the prior probability before the toss i+1, so the distinction between prior and posterior depends on which trial we are referring to. Both describe our incomplete knowledge of how the tossed coin is going to land, and they differ only by the amount of information we incorporated into updating that knowledge.

Notice how Bayesian inference of the most likely probabilistic model does not rely at all on the frequentist approach to the probability. Contrary to the latter, the Bayesian approach allows us to make quantitative judgements / logical inferences without resorting to the requirement of the large number of trials. Importantly, in the Bayesian approach the frequency/average (which is an observable) is determined from the probability (which is not an observable), not vice versa.

The correct understanding of probability in the Bayesian sense becomes crucial for the correct understaning of the fundamentally existing probabilistic nature of quantum mechanics. In the latter the probability is elevated to the complex-valued and dynamical wave function (also referred to as the probability wave or the amplitude of probability), but its probabilistic nature itself is still the same as in the classical Bayesian probability. In particular, the wave function also encodes incomplete knowledge of the observer.

Notice that just like the probability itself is not an observable (instead, we observe averages, dispersion, correlation functions, higher moments of the probability distribution, etc), so is the wave function not an observable (instead we observe average/expectatation values of operators, densities, correlations, etc).

Therefore, in particular, collapse of the wave function (which is also and in particular not an observable process) reflects the change of knowledge of the observer, who measured some given operator in one of its eigenstates. In other words, collapse of the wave function does not correspond to any dynamics in the observed system itself, but only to the update of knowledge of the observer / classical measuring device. 
